# RAGTruth Benchmark

This benchmark is based on the [RAGTruth](https://github.com/ParticleMedia/RAGTruth) dataset, a hallucination detection benchmark tailored for Retrieval-Augmented Generation (RAG) systems. The dataset consists of three task types: **Summarization**, **Data-to-Text Generation**, and **Question Answering (QA)**. Each task includes human-written source documents paired with responses generated by multiple Large Language Models (LLMs).

## Tasks

RAGTruth includes 2,965 unique source tasks:

* **943** for Summarization
* **1,033** for Data-to-Text
* **989** for Question Answering

For each task, answers are generated using **6 different LLMs**, resulting in a total of **17,790 documents**:

* **5,658** Summarization responses
* **6,198** Data-to-Text responses
* **5,934** QA responses

These generated responses are located in the `dataset/` folder and are split into `training_data.json` and `response.json` (test) sets.

## Splits

* The **test split** consists of **2,700** LLM-generated answers (900 per task type).
* The remaining samples are in the **train split**.

## Evaluation Setup

Following the official paper [RAGTruth instructions](https://arxiv.org/abs/2401.00396), we generated **10 samples per LLM-generated answer** in the test set. These synthetic samples enable hallucination detection evaluation using methods such as:

* **CheckEmbed**
* [**SelfCheckGPT**](https://github.com/potsawee/selfcheckgpt)

The code for generating samples is included in the `samples/` directory. All generations were run locally, with **no additional inference cost**.

## Baselines

We also include an adjusted version of [**HalluDetect**](https://github.com/Baylor-AI/HalluDetect), an effective method for hallucination classification in text generation.

> **Note:** HalluDetect requires significant computational resources. For optimal performance, we recommend at least **2 Ã— NVIDIA A100 GPUs** (40GB+ VRAM).

## Cost Estimation

* **LLM-as-a-Judge**: \~\$20 total for scoring using default LLMs.

## How to Run

To reproduce the experiments:

```bash
cd examples/RAGTruth/dataset
# (Optional) Run sampling script if not already done
# The samples are already given alongside the repository
python3 sampler.py

# Evaluate using CE
cd examples/RAGTruth
python3 main.py

# Run HalluDetect
cd examples/RAGTruth
python3 hallu_detect.py
```
